{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/nejm-brain-to-text/model_training/train_baseline_model.py\n",
    "from omegaconf import OmegaConf\n",
    "from rnn_trainer import BrainToTextDecoder_Trainer\n",
    "import os\n",
    "import numpy as np\n",
    "import torch \n",
    "import shutil\n",
    "\n",
    "args_path = 'rnn_args.yaml'\n",
    "if not os.path.exists(args_path):\n",
    "    print(f\"Warning: '{args_path}' not found. Using pretrained model's args as template.\")\n",
    "    args_path = \"/kaggle/input/brain-to-text-25/t15_pretrained_rnn_baseline/t15_pretrained_rnn_baseline/checkpoint/args.yaml\"\n",
    "    \n",
    "args = OmegaConf.load(args_path)\n",
    "\n",
    "# Force the script to use the correct Kaggle data paths\n",
    "args.dataset.dataset_dir = \"/kaggle/working/brain-to-text-25-minimal-shirley/t15_copyTask_neuralData/hdf5_data_final\"\n",
    "\n",
    "# Train on first 3 sessions, reserve 4th for testing\n",
    "args.dataset.sessions = [\n",
    "    't15.2023.08.11',\n",
    "    't15.2023.08.13',\n",
    "    't15.2023.08.18',\n",
    "    't15.2023.08.20'\n",
    "]\n",
    "\n",
    "# Update n_days to match number of sessions\n",
    "args.model.n_days = len(args.dataset.sessions)\n",
    "\n",
    "# Update dataset probabilities\n",
    "# Note: t15.2023.08.11 has no val data, so set its val probability to 0\n",
    "if hasattr(args.dataset, 'dataset_probability_val'):\n",
    "    # Set to 0 for days without validation data\n",
    "    args.dataset.dataset_probability_val = [0.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "# Disable logging of individual day validation PER to avoid division by zero\n",
    "args.log_individual_day_val_PER = False\n",
    "\n",
    "args.num_training_batches = 10000\n",
    "print(f\"\\nTraining for {args.num_training_batches} batches.\")\n",
    "args.days_per_batch = 2\n",
    "print(f\"\\nTraining for {args.days_per_batch} days per batches.\")\n",
    "args.batch_size = 16\n",
    "print(f\"\\nTraining for batch size = {args.batch_size}.\")\n",
    "\n",
    "\n",
    "# Set output directories\n",
    "new_output_dir = \"trained_models/baseline_rnn/progressive_training\"\n",
    "new_checkpoint_dir = \"trained_models/baseline_rnn/checkpoint/progressive_training\"\n",
    "args.output_dir = new_output_dir\n",
    "args.checkpoint_dir = new_checkpoint_dir\n",
    "\n",
    "# Remove old directories if they exist\n",
    "if os.path.exists(args.output_dir):\n",
    "    print(f\"Removing existing output directory: {args.output_dir}\")\n",
    "    shutil.rmtree(args.output_dir) \n",
    "if os.path.exists(args.checkpoint_dir):\n",
    "    print(f\"Removing existing checkpoint directory: {args.checkpoint_dir}\")\n",
    "    shutil.rmtree(args.checkpoint_dir) \n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Training sessions: {args.dataset.sessions}\")\n",
    "print(f\"  Training: All 4 sessions using data_train.hdf5\")\n",
    "print(f\"  Validation: Sessions 2-4 using data_val.hdf5 (Session 1 has no val data)\")\n",
    "print(f\"  Reserved: data_test.hdf5 from all sessions (for future testing)\")\n",
    "print(f\"  Optimizer: AdamW (default)\")\n",
    "print(f\"  Learning rate: {args.lr_max}\")\n",
    "print(f\"  Batch size: {args.dataset.batch_size}\")\n",
    "print(f\"  Output dir: {args.output_dir}\")\n",
    "\n",
    "# Verify data files exist\n",
    "print(\"\\nVerifying data files:\")\n",
    "for i, session in enumerate(args.dataset.sessions):\n",
    "    train_path = os.path.join(args.dataset.dataset_dir, session, 'data_train.hdf5')\n",
    "    val_path = os.path.join(args.dataset.dataset_dir, session, 'data_val.hdf5')\n",
    "    test_path = os.path.join(args.dataset.dataset_dir, session, 'data_test.hdf5')\n",
    "    \n",
    "    train_exists = os.path.exists(train_path)\n",
    "    val_exists = os.path.exists(val_path)\n",
    "    test_exists = os.path.exists(test_path)\n",
    "    \n",
    "    if not train_exists:\n",
    "        print(f\"    WARNING: Missing training data!\")\n",
    "\n",
    "# CREATE TRAINER\n",
    "print(\"\\nInitializing trainer...\")\n",
    "trainer = BrainToTextDecoder_Trainer(args) \n",
    "\n",
    "# RUN TRAINING\n",
    "print(\"Starting base model training...\")\n",
    "train_stats = trainer.train() \n",
    "val_per_list = train_stats.get('val_PERs', []) \n",
    "val_score = np.min(val_per_list) if val_per_list else 1.0 \n",
    "print(f\"Base Model Training Finished\")\n",
    "print(f\"Final Best (min) Validation PER: {val_score:.4f}\")\n",
    "print(f\"Model checkpoint saved in: {args.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /kaggle/working/nejm-brain-to-text/model_training/ && \\\n",
    "python train_baseline_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "eval_script = \"/kaggle/working/nejm-brain-to-text/model_training/evaluate_sessions.py\"\n",
    "model_path = \"/kaggle/working/nejm-brain-to-text/model_training/trained_models/baseline_rnn/checkpoint/progressive_training\"\n",
    "data_dir = \"/kaggle/working/brain-to-text-25-minimal-shirley/t15_copyTask_neuralData/hdf5_data_final\"\n",
    "eval_type = \"test\"\n",
    "gpu_number = 0\n",
    "\n",
    "target_sessions = [\"t15.2023.08.13\", \"t15.2023.08.18\", \"t15.2023.08.20\"]\n",
    "sessions_str = \" \".join(target_sessions)\n",
    "cmd = f\"\"\"\n",
    "cd /kaggle/working/nejm-brain-to-text/model_training/ && \\\n",
    "python {eval_script} \\\n",
    "    --model_path {model_path} \\\n",
    "    --data_dir {data_dir} \\\n",
    "    --eval_type {eval_type} \\\n",
    "    --gpu_number {gpu_number} \\\n",
    "    --sessions {sessions_str}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Eval type: {eval_type}\")\n",
    "print(\"\\nEvaluating on:\")\n",
    "for session in target_sessions:\n",
    "    print(f\"Evaluating {session}: data_{eval_type}.hdf5\")\n",
    "os.system(cmd)\n",
    "print(\"Evaluation finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def analyze_predictions(output_file):\n",
    "    df = pd.read_csv(output_file)\n",
    "    \n",
    "    overall_avg = df['trial_acc'].mean()\n",
    "    print(f\"Overall average trial accuracy: {overall_avg:.4f}\")\n",
    "\n",
    "    # session_avg = df.groupby('session')['trial_acc'].mean()\n",
    "    # print(\"\\nAverage accuracy per session:\")\n",
    "    # print(session_avg)\n",
    "\n",
    "    # Calculate AGGREGATE PER\n",
    "    total_edit_distance = 0\n",
    "    total_phoneme_length = 0\n",
    "    for index, row in df.iterrows():\n",
    "        pred_seq = row['true_phoneme'].split('-')\n",
    "        true_seq = row['true_phoneme'].split('-')\n",
    "\n",
    "        ed = row['trial_ed']\n",
    "        true_len = len(true_seq)\n",
    "        \n",
    "        total_edit_distance += ed\n",
    "        total_phoneme_length += true_len\n",
    "            \n",
    "    aggregate_per = total_edit_distance / total_phoneme_length\n",
    "    print(f\"Aggregate Phoneme Error Rate (PER): {aggregate_per:.4f}\")\n",
    "        \n",
    "    avg_loss = df['trial_ctc_loss'].mean()\n",
    "    print(f\"Average Validation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Open the csv and perform some analysis on it\n",
    "output_file = \"/kaggle/working/nejm-brain-to-text/model_training/output/phoneme_predictions_20251105_065010.csv\"\n",
    "df = pd.read_csv(output_file)\n",
    "df.head()\n",
    "analyze_predictions(output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
